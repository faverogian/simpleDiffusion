{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple diffusion: End-to-end diffusion for high resolution images\n",
    "### Unofficial PyTorch Implementation by Gian Favero\n",
    "\n",
    "**Simple diffusion: End-to-end diffusion for high resolution images**  \n",
    "[Emiel Hoogeboom](https://arxiv.org/search/cs?searchtype=author&query=Hoogeboom,+E), [Jonathan Heek](https://arxiv.org/search/cs?searchtype=author&query=Heek,+J), [Tim Salimans](https://arxiv.org/search/cs?searchtype=author&query=Salimans,+T)\n",
    "https://arxiv.org/abs/2301.11093\n",
    "\n",
    "GitHub Repository: https://github.com/faverogian/simpleDiffusion/blob/main/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.special import expm1\n",
    "import math\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from ema_pytorch import EMA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# helpers\n",
    "def log(t, eps = 1e-20):\n",
    "    return torch.log(t.clamp(min = eps))\n",
    "\n",
    "def clip(x):\n",
    "    \"\"\"\n",
    "    Function to clip the input tensor x to the range [-1, 1].\n",
    "\n",
    "    Args:\n",
    "    x (torch.Tensor): The input tensor to clip.\n",
    "\n",
    "    Returns:\n",
    "    x (torch.Tensor): The clipped tensor.\n",
    "    \"\"\"\n",
    "    return torch.clamp(x, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Diffusion\n",
    "We define a class that allows the training of a diffusion model using the simple diffusion paradigm - highlighted by the introduction of a shifted cosine noise schedule. The intuition is relatively simple: find a noise schedule that works at a baseline resolution and shift it proportionally to a new image size.\n",
    "\n",
    "For example, we know from a littany of previous experiments in literature that a cosine noise schedule works well on 64x64 images, but fails to scale effectively to images of larger size. Instead of sending up a prayer and hoping that the same results can be achieved on larger images (say 256x256), we shift the noise schedule proportionally to the differnce in image size, leading to more consistent results across the board.\n",
    "\n",
    "A sample class definition is seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        unet,\n",
    "        image_size,\n",
    "        noise_size=64,\n",
    "        pred_param='v', \n",
    "        schedule='shifted_cosine', \n",
    "        steps=512\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Training objective\n",
    "        assert pred_param in ['v', 'eps'], \"Invalid prediction parameterization. Must be 'v' or 'eps'\"\n",
    "        self.pred_param = pred_param\n",
    "\n",
    "        # Sampling schedule\n",
    "        assert schedule in ['cosine', 'shifted_cosine'], \"Invalid schedule. Must be 'cosine' or 'shifted_cosine'\"\n",
    "        self.schedule = schedule\n",
    "        self.noise_d = noise_size\n",
    "        self.image_d = image_size\n",
    "\n",
    "        # Model\n",
    "        assert isinstance(unet, nn.Module), \"Model must be an instance of torch.nn.Module.\"\n",
    "        self.model = unet\n",
    "\n",
    "        num_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(f\"Number of parameters: {num_params}\")\n",
    "\n",
    "        # Steps\n",
    "        self.steps = steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Forward Diffusion Process\n",
    "\n",
    "Since we are working with Gaussian noise in a Markovian manner, a noised image at any time $t$ can be derived from a base image, $z_t \\sim p(z_t | z_s, x)$, where $0 \\leq s \\leq t \\leq 1$. Due to the properties of Gaussian noise addition, we do not have to repeatedly sample from $p(z_t | z_s, x)$, but rather directly arrive at $z_t$ from the base image $x$. This forward process can be defined as: \n",
    "\\begin{align*}\n",
    "    \\bm{z}_t = \\alpha_t \\bm{x} + \\sigma_t \\bm{\\epsilon}; \\quad \\bm{\\epsilon} \\sim \\mathcal{N}(\\bm{\\epsilon}; \\bm{0}, \\bm{\\text{I}})\n",
    "\\end{align*}\n",
    "In the above Gaussian noising process $\\alpha_t$ and $\\sigma_t$ are strictly positive scalar-valued functions of $t \\in [0,1]$. We implement a method called \"diffuse\" to carry out this forward process for a given $\\alpha_t$ and $\\sigma_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse(x, alpha_t, sigma_t):\n",
    "    \"\"\"\n",
    "    Function to diffuse the input tensor x to a timepoint t with the given alpha_t and sigma_t.\n",
    "\n",
    "    Args:\n",
    "    x (torch.Tensor): The input tensor to diffuse.\n",
    "    alpha_t (torch.Tensor): The alpha value at timepoint t.\n",
    "    sigma_t (torch.Tensor): The sigma value at timepoint t.\n",
    "\n",
    "    Returns:\n",
    "    z_t (torch.Tensor): The diffused tensor at timepoint t.\n",
    "    eps_t (torch.Tensor): The noise tensor at timepoint t.\n",
    "    \"\"\"\n",
    "    eps_t = torch.randn_like(x)\n",
    "\n",
    "    z_t = alpha_t * x + sigma_t * eps_t\n",
    "\n",
    "    return z_t, eps_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Log-SNR Schedule\n",
    "\n",
    "In the above Gaussian noising process $\\alpha_t$ and $\\sigma_t$ are related through a signal-to-noise ratio in the image. The $\\log$-SNR is additionally defined as $\\lambda = \\log \\frac{\\alpha_t^2}{\\sigma_t^2}$, where $\\lambda$ is strictly monotonically decreasing in time, such that $\\lambda_{\\max}$ occurs at $t=0$ and $\\lambda_{\\min}$ occurs at $t=1$ so that $\\bm{z}_T = \\mathcal{N}(\\bm{0}, \\bm{\\text{I}})$. At a given timepoint $t$, we use a function $\\lambda = f_\\lambda (t)$ from which we obtain $\\alpha_t$ and $\\sigma_t$. The forward process (or the destruction of $\\bm{x}$) is commonly defined to be variance-preserving, imposing the constraint $\\alpha_t^2 + \\sigma_t^2 = 1$. This implies:\n",
    "\\begin{align*}\n",
    "    \\alpha(t)^2 &= \\text{sigmoid}(\\lambda) \\\\\n",
    "    \\sigma(t)^2 &= \\text{sigmoid}(-\\lambda)\n",
    "\\end{align*}\n",
    "\n",
    "There are many schools of thought as to the best noise schedule for training diffusion models at various resolutions and image types (natural images, medical, etc.). A common choice is the cosine noise schedule as introduced by Dhariwal and Nichol in their \"Diffusion Models beat GANs on Image Synthesis\" paper:\n",
    "\\begin{align*}\n",
    "    \\lambda(t) &= \\log \\frac{\\alpha_t^2}{\\sigma_t^2} \\\\\n",
    "    &= -2 \\log (\\tan (\\pi t / 2))\n",
    "\\end{align*}\n",
    "We implement this in the method below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsnr_schedule_cosine(self, t, logsnr_min=-15, logsnr_max=15):\n",
    "    \"\"\"\n",
    "    Function to compute the logSNR schedule at timepoint t with cosine:\n",
    "\n",
    "    logSNR(t) = -2 * log (tan (pi * t / 2))\n",
    "\n",
    "    Taking into account boundary effects, the logSNR value at timepoint t is computed as:\n",
    "\n",
    "    logsnr_t = -2 * log(tan(t_min + t * (t_max - t_min)))\n",
    "\n",
    "    Args:\n",
    "    t (int): The timepoint t.\n",
    "    logsnr_min (int): The minimum logSNR value.\n",
    "    logsnr_max (int): The maximum logSNR value.\n",
    "\n",
    "    Returns:\n",
    "    logsnr_t (float): The logSNR value at timepoint t.\n",
    "    \"\"\"\n",
    "    logsnr_max = logsnr_max + math.log(self.noise_d / self.image_d)\n",
    "    logsnr_min = logsnr_min + math.log(self.noise_d / self.image_d)\n",
    "    t_min = math.atan(math.exp(-0.5 * logsnr_max))\n",
    "    t_max = math.atan(math.exp(-0.5 * logsnr_min))\n",
    "\n",
    "    logsnr_t = -2 * log(torch.tan(torch.tensor(t_min + t * (t_max - t_min))))\n",
    "\n",
    "    return logsnr_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Shifted-Cosine Noise Schedule\n",
    "\n",
    "As demonstrated in \"End-to-end diffusion for high resolution images\" by Hoogeboom et al., more efficient training of diffusion models can be achieved when the noise schedule is adjusted for the image resolution. In the log-space, a proportional adjustment can be obtained by:\n",
    "\\begin{align*}\n",
    "    \\lambda_{\\text{shifted}}(t) = \\lambda(t) + 2 \\log \\left( \\frac{\\text{base dimension}}{\\text{image dimension}} \\right)\n",
    "\\end{align*}\n",
    "We implement this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsnr_schedule_cosine_shifted(self, t):\n",
    "    \"\"\"\n",
    "    Function to compute the logSNR schedule at timepoint t with shifted cosine:\n",
    "\n",
    "    logSNR_shifted(t) = logSNR(t) + 2 * log(noise_d / image_d)\n",
    "\n",
    "    Args:\n",
    "    t (int): The timepoint t.\n",
    "    image_d (int): The image dimension.\n",
    "    noise_d (int): The noise dimension.\n",
    "\n",
    "    Returns:\n",
    "    logsnr_t_shifted (float): The logSNR value at timepoint t.\n",
    "    \"\"\"\n",
    "    logsnr_t = self.logsnr_schedule_cosine(t)\n",
    "    logsnr_t_shifted = logsnr_t + 2 * math.log(self.noise_d / self.image_d)\n",
    "\n",
    "    return logsnr_t_shifted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Loss Function\n",
    "\n",
    "Training a diffusion model is a repeated process of:\n",
    "1. Add noise to an image according to a given timestep, $t$\n",
    "2. Predict (by some parameterization) the noise that was added to the original image\n",
    "\n",
    "We can attempt to predict the noise that was added to the image in various ways. Common choices of parameterization are $x$-, $\\epsilon$- and $v$-prediction. Given $\\bm{z}_t = \\alpha_t \\bm{x} + \\sigma_t \\bm{\\epsilon}$, an $\\epsilon$-prediction model tries to directly estimate the noise, while an $x$-prediction model attempts to directly recover the original image. A more modern take that came alongside progressive distillation (Ho et al.) is $v$-prediction, which estimates both the original image and the noise added: $v = \\alpha_t \\bm{x} + \\sigma_t \\epsilon$, finding benefits in stability.\n",
    "\n",
    "All three are related, though imply various weightings to the loss function. Thus, they are not interchangeable, but rather a means to the same end. In our loss function, we provide the option to use a (shifted) cosine noise schedule, as well as the option to choose epsilon- or v-prediction. The loss is implemented as an optimization of an MSE of the parameterization of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self, x):\n",
    "    \"\"\"\n",
    "    A function to compute the loss of the model. The loss is computed as the mean squared error\n",
    "    between the predicted noise tensor and the true noise tensor. Various prediction parameterizations\n",
    "    imply various weighting schemes as outlined in Kingma et al. (2023)\n",
    "\n",
    "    Args:\n",
    "    x (torch.Tensor): The input tensor.\n",
    "\n",
    "    Returns:\n",
    "    loss (torch.Tensor): The loss value.\n",
    "    \"\"\"\n",
    "    t = torch.rand(x.shape[0])\n",
    "\n",
    "    if self.schedule == 'cosine':\n",
    "        logsnr_t = self.logsnr_schedule_cosine(t)\n",
    "    elif self.schedule == 'shifted_cosine':\n",
    "        logsnr_t = self.logsnr_schedule_cosine_shifted(t)\n",
    "\n",
    "    logsnr_t = logsnr_t.to(x.device)\n",
    "    alpha_t = torch.sqrt(torch.sigmoid(logsnr_t)).view(-1, 1, 1, 1).to(x.device)\n",
    "    sigma_t = torch.sqrt(torch.sigmoid(-logsnr_t)).view(-1, 1, 1, 1).to(x.device)\n",
    "    z_t, eps_t = self.diffuse(x, alpha_t, sigma_t)\n",
    "    pred = self.model(z_t, logsnr_t)\n",
    "\n",
    "    if self.pred_param == 'v':\n",
    "        eps_pred = sigma_t * z_t + alpha_t * pred\n",
    "    else: \n",
    "        eps_pred = pred\n",
    "\n",
    "    # Apply min-SNR weighting (https://arxiv.org/pdf/2303.09556)\n",
    "    snr = torch.exp(logsnr_t).clamp_(max = 5)\n",
    "    if self.pred_param == 'v':\n",
    "        weight = 1 / (1 + snr)\n",
    "    else:\n",
    "        weight = 1 / snr\n",
    "\n",
    "    weight = weight.view(-1, 1, 1, 1)\n",
    "\n",
    "    loss = torch.mean(weight * (eps_pred - eps_t) ** 2)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating $\\epsilon_0$, $x_0$, or $v_0$ from pure Gaussian noise is a relatively impossible task and the same simplification done in the forward diffusion process is not achievable in the reverse process when sampling. In theory, the forward process takes its course over a series of timesteps, typically between 512 and 1000, which through a Markovian lens is a repeated samping of $p(z_t | z_s, x)$ where $0 \\leq s \\leq t \\leq 1$. In the reverse process, we repeatedly sample $z_s \\sim p(z_s | z_t)$ over the same set of timesteps before arriving at a sampled image.\n",
    "\n",
    "The following sampling step algorithm is derived from Gaussian math and is shown beautifully in Appendix A.4 of \"Variational Diffusion Models\" by Kingma et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ddpm_sampler_step(self, z_t, pred, logsnr_t, logsnr_s):\n",
    "    \"\"\"\n",
    "    Function to perform a single step of the DDPM sampler.\n",
    "\n",
    "    Args:\n",
    "    z_t (torch.Tensor): The diffused tensor at timepoint t.\n",
    "    pred (torch.Tensor): The predicted value from the model (v or eps).\n",
    "    logsnr_t (float): The logSNR value at timepoint t.\n",
    "    logsnr_s (float): The logSNR value at the sampling timepoint s.\n",
    "\n",
    "    Returns:\n",
    "    z_s (torch.Tensor): The diffused tensor at sampling timepoint s.\n",
    "    \"\"\"\n",
    "    c = -expm1(logsnr_t - logsnr_s)\n",
    "    alpha_t = torch.sqrt(torch.sigmoid(logsnr_t))\n",
    "    alpha_s = torch.sqrt(torch.sigmoid(logsnr_s))\n",
    "    sigma_t = torch.sqrt(torch.sigmoid(-logsnr_t))\n",
    "    sigma_s = torch.sqrt(torch.sigmoid(-logsnr_s))\n",
    "\n",
    "    if self.pred_param == 'v':\n",
    "        x_pred = alpha_t * z_t - sigma_t * pred\n",
    "    elif self.pred_param == 'eps':\n",
    "        x_pred = (z_t - sigma_t * pred) / alpha_t\n",
    "\n",
    "    x_pred = self.clip(x_pred)\n",
    "\n",
    "    mu = alpha_s * (z_t * (1 - c) / alpha_t + c * x_pred)\n",
    "    variance = (sigma_s ** 2) * c\n",
    "\n",
    "    return mu, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of the above algorithm by repeatedly calling it in a loop over T timesteps. Beginning with pure Gaussian noise, we slowly recover an image, one step at a time, using stochasticity as a self-correction technique as we know predicting an image in one-shot is nearly impossible for modern U-Nets (or to train such a model in a reasonable time frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(self, x):\n",
    "    \"\"\"\n",
    "    Standard DDPM sampling procedure. Begun by sampling z_T ~ N(0, 1)\n",
    "    and then repeatedly sampling z_s ~ p(z_s | z_t)\n",
    "\n",
    "    Args:\n",
    "    x_shape (tuple): The shape of the input tensor.\n",
    "\n",
    "    Returns:\n",
    "    x_pred (torch.Tensor): The predicted tensor.\n",
    "    \"\"\"\n",
    "    z_t = torch.randn(x.shape).to(x.device)\n",
    "\n",
    "    # Steps T -> 1\n",
    "    for t in reversed(range(1, self.steps+1)):\n",
    "        u_t = t / self.steps\n",
    "        u_s = (t - 1) / self.steps\n",
    "\n",
    "        if self.schedule == 'cosine':\n",
    "            logsnr_t = self.logsnr_schedule_cosine(u_t)\n",
    "            logsnr_s = self.logsnr_schedule_cosine(u_s)\n",
    "        elif self.schedule == 'shifted_cosine':\n",
    "            logsnr_t = self.logsnr_schedule_cosine_shifted(u_t)\n",
    "            logsnr_s = self.logsnr_schedule_cosine_shifted(u_s)\n",
    "\n",
    "        logsnr_t = logsnr_t.to(x.device)\n",
    "        logsnr_s = logsnr_s.to(x.device)\n",
    "\n",
    "        pred = self.model(z_t, logsnr_t)\n",
    "        mu, variance = self.ddpm_sampler_step(z_t, pred, torch.tensor(logsnr_t), torch.tensor(logsnr_s))\n",
    "        z_t = mu + torch.randn_like(mu) * torch.sqrt(variance)\n",
    "\n",
    "    # Final step\n",
    "    if self.schedule == 'cosine':\n",
    "        logsnr_1 = self.logsnr_schedule_cosine(1/self.steps)\n",
    "        logsnr_0 = self.logsnr_schedule_cosine(0)\n",
    "    elif self.schedule == 'shifted_cosine':\n",
    "        logsnr_1 = self.logsnr_schedule_cosine_shifted(1/self.steps)\n",
    "        logsnr_0 = self.logsnr_schedule_cosine_shifted(0)\n",
    "\n",
    "    logsnr_1 = logsnr_1.to(x.device)\n",
    "    logsnr_0 = logsnr_0.to(x.device)\n",
    "\n",
    "    pred = self.model(z_t, logsnr_1)\n",
    "    x_pred, _ = self.ddpm_sampler_step(z_t, pred, torch.tensor(logsnr_1), torch.tensor(logsnr_0))\n",
    "    \n",
    "    x_pred = clip(x_pred)\n",
    "\n",
    "    # Convert x_pred to the range [0, 1]\n",
    "    x_pred = (x_pred + 1) / 2\n",
    "\n",
    "    return x_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Training Loop\n",
    "\n",
    "From here the process is fairly straightforward. We optimize the loss function of the model, typically chosen as a U-Net. The U-Net (conditioned on $t$) is taken as a noise predictor network is trained over a set of timesteps (again, typically 512 to 1000). Unfortunately, tracking the loss function during training is not so transparent with diffusion experiments. Training progress is typically monitored by evaluating samples at regular intervals through visual metrics or comparative ones such as FID.\n",
    "\n",
    "In this training loop, distributed training and gradient accumulation is implemented via the HuggingFace Accelerate library, while EMA tracking is done via the ema_pytorch library. Sample usage in a training script can be found at the GitHub page for this project: https://github.com/faverogian/simpleDiffusion/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(self, config, optimizer, train_dataloader, lr_scheduler):\n",
    "    \"\"\"\n",
    "    A function to train the model.\n",
    "\n",
    "    Args:\n",
    "    optimizer (torch.optim.Optimizer): The optimizer to use for training.\n",
    "    \"\"\"\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        if config.push_to_hub:\n",
    "            repo_id = create_repo(\n",
    "                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True\n",
    "            ).repo_id\n",
    "\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare( \n",
    "        self.model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    # Create an EMA model\n",
    "    ema = EMA(\n",
    "        model,\n",
    "        beta=0.9999,\n",
    "        update_after_step=100,\n",
    "        update_every=10\n",
    "    )\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            x = batch[\"images\"]\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                loss = self.loss(x)\n",
    "                loss = loss.to(next(model.parameters()).dtype)\n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Update EMA model parameters\n",
    "            ema.update()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # After each epoch you optionally sample some demo images\n",
    "        if accelerator.is_main_process:\n",
    "            self.model = accelerator.unwrap_model(model)\n",
    "            self.model.eval()\n",
    "\n",
    "        # Make directory for saving images\n",
    "            os.makedirs(os.path.join(config.output_dir, \"images\"), exist_ok=True)\n",
    "\n",
    "            if epoch % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                sample = self.sample(x[0].unsqueeze(0))\n",
    "                sample = sample.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "                image_path = os.path.join(config.output_dir, \"images\", f\"sample_{epoch}.png\")\n",
    "                plt.imsave(image_path, sample[0])\n",
    "    \n",
    "            # Save the EMA model to HuggingFace Hub\n",
    "            if config.push_to_hub and epoch == config.num_epochs - 1:\n",
    "                upload_folder(\n",
    "                    repo_id=repo_id,\n",
    "                    folder_path=config.output_dir,\n",
    "                    commit_message=\"EMA model\",\n",
    "                )\n",
    "                self.model.push_to_hub(config.hub_model_id, variant=\"fp16\")\n",
    "                torch.save(ema.ema_model.module.state_dict(), 'ema_model.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
